{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5eb387d0",
      "metadata": {
        "id": "5eb387d0"
      },
      "source": [
        "# Artificial Intelligence for Complex Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b74dd7",
      "metadata": {
        "id": "60b74dd7"
      },
      "source": [
        "## How to build a profanity detector: Two approaches\n",
        "\n",
        "The best way to see how AI works in practice is to try it. In this class, we're going to look at a classic use case for machine learning: detecting profanity in written text. First, we'll try to do this using *without* using machine learning. Then we'll try it using machine learning and see if theres's a difference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/texturejc/AI_for_complex_problems\n"
      ],
      "metadata": {
        "id": "wFU_IVQJ5e39"
      },
      "id": "wFU_IVQJ5e39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef7254f",
      "metadata": {
        "id": "4ef7254f"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Codde to import relevant python libraries\n",
        "import pandas as pd #data science library\n",
        "import plotly.express as px #visualisation library\n",
        "import plotly.graph_objects as go\n",
        "import scipy #scientific computing library\n",
        "from scipy.spatial import distance\n",
        "import random\n",
        "\n",
        "#Linguistic data that we'll need\n",
        "data = pd.read_csv('/content/AI_for_complex_problems/VAD_rescale.csv', index_col = 'word')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfc2008",
      "metadata": {
        "id": "dbfc2008"
      },
      "source": [
        "## Approach 1: Creating a hypothesis concerning the nature of profanity and using that to predict what words are likely to count as profane\n",
        "\n",
        "Exercise: everyone take five minutes to come up with the linguistic features of profanity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da51e51a",
      "metadata": {
        "id": "da51e51a"
      },
      "outputs": [],
      "source": [
        "data = data[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
        "data.columns = ['valence', 'arousal', 'dominance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bddb9f",
      "metadata": {
        "id": "f8bddb9f"
      },
      "outputs": [],
      "source": [
        "emotions = ['sadness', 'anger', 'happiness', 'depression', 'disgust', 'fear', 'surprise', 'hysteria'] #add to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31b5eda",
      "metadata": {
        "id": "f31b5eda"
      },
      "outputs": [],
      "source": [
        "emo_words = []\n",
        "\n",
        "for i in emotions:\n",
        "    if i in data.index:\n",
        "        emo_words.append(i)\n",
        "\n",
        "emo_df = data.loc[emo_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6831b63",
      "metadata": {
        "id": "f6831b63"
      },
      "outputs": [],
      "source": [
        "emo_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c263c525",
      "metadata": {
        "id": "c263c525"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.scatter_3d(emo_df, x='valence', y='arousal', z='dominance',\n",
        "               hover_data = [emo_df.index])\n",
        "\n",
        "fig.update_traces(marker=dict(size = 8, line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c250d97e",
      "metadata": {
        "id": "c250d97e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(src='https://texturejc.github.io/qual1a/all_vad_words.html', width=1050, height=1050)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e920827b",
      "metadata": {
        "id": "e920827b"
      },
      "outputs": [],
      "source": [
        "bad_words = ['fuck', 'shit', 'asshole', 'dickhead', 'moron']\n",
        "\n",
        "sample = ['confident', 'christmas', 'kitten', 'prize', 'apple']\n",
        "\n",
        "test_words = bad_words + sample\n",
        "\n",
        "test_df = data.loc[test_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aba68cc",
      "metadata": {
        "id": "0aba68cc"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(test_df, x='valence', y='arousal', z='dominance',\n",
        "               hover_data = [test_df.index])\n",
        "\n",
        "fig.update_traces(marker=dict(size = 9, line=dict(width=2,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "\n",
        "for i in bad_words:\n",
        "    for j in bad_words:\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "            x=[test_df.loc[i]['valence'], test_df.loc[j]['valence']],\n",
        "            y=[test_df.loc[i]['arousal'], test_df.loc[j]['arousal']],\n",
        "            z=[test_df.loc[i]['dominance'], test_df.loc[j]['dominance']],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', width=2),\n",
        "            )\n",
        "            )\n",
        "\n",
        "\n",
        "for i in sample:\n",
        "    for j in sample:\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "            x=[test_df.loc[i]['valence'], test_df.loc[j]['valence']],\n",
        "            y=[test_df.loc[i]['arousal'], test_df.loc[j]['arousal']],\n",
        "            z=[test_df.loc[i]['dominance'], test_df.loc[j]['dominance']],\n",
        "            mode='lines',\n",
        "            line=dict(color='blue', width=2)\n",
        "            )\n",
        "            )\n",
        "\n",
        "for i in bad_words:\n",
        "    for j in sample:\n",
        "        fig.add_trace(\n",
        "            go.Scatter3d(\n",
        "            x=[test_df.loc[i]['valence'], test_df.loc[j]['valence']],\n",
        "            y=[test_df.loc[i]['arousal'], test_df.loc[j]['arousal']],\n",
        "            z=[test_df.loc[i]['dominance'], test_df.loc[j]['dominance']],\n",
        "            mode='lines',\n",
        "            line=dict(color='green', width=2)\n",
        "            )\n",
        "            )\n",
        "fig.update_traces(showlegend=False)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be971c4a",
      "metadata": {
        "id": "be971c4a"
      },
      "outputs": [],
      "source": [
        "dist_1 = distance.euclidean(data.loc['asshole'], data.loc['confident'])\n",
        "dist_2 = distance.euclidean(data.loc['asshole'], data.loc['dickhead'])\n",
        "\n",
        "print(dist_1, dist_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b433082",
      "metadata": {
        "id": "6b433082"
      },
      "outputs": [],
      "source": [
        "bad_word = 'fuck'\n",
        "words = []\n",
        "dist = []\n",
        "\n",
        "\n",
        "for i in data.index:\n",
        "    dist.append(distance.euclidean(data.loc[i], data.loc[bad_word]))\n",
        "    words.append(i)\n",
        "\n",
        "profane = pd.Series(dist, index = words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce73ebe2",
      "metadata": {
        "id": "ce73ebe2"
      },
      "outputs": [],
      "source": [
        "profane = profane.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4996432a",
      "metadata": {
        "id": "4996432a"
      },
      "outputs": [],
      "source": [
        "prof_small = profane.head(n = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "526022f0",
      "metadata": {
        "id": "526022f0"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(x = prof_small.index, y = prof_small)\n",
        "fig.layout.yaxis.title = \"distance from {}\".format(bad_word)\n",
        "fig.layout.xaxis.title = \"word\"\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef92c5f",
      "metadata": {
        "id": "3ef92c5f"
      },
      "source": [
        "### What are the problems of this approach?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d53a3f2",
      "metadata": {
        "id": "9d53a3f2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ed3cf685",
      "metadata": {
        "id": "ed3cf685"
      },
      "source": [
        "## Approach 2: Training a machine learning algorithm to predict profanity\n",
        "\n",
        "Exercise: everyone take five minutes to come up with the linguistic features of profanity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4887e7d6",
      "metadata": {
        "id": "4887e7d6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7317ba4b",
      "metadata": {
        "id": "7317ba4b"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "Linear regression based machine learning methods are used to predict numerical outputs. That is, based on the paramteters estimated from its training data, the algorithm takes a series of inputs and predicts the estimated value of their output. However, whilst this method is very useful, there are many situations where we want to predict a *category* rather than a number. For example, we may wish to estimate whether the data predicts whether or not test is passed based on hours studied, or what species of animal is represented based on features in an image, or whether a customer is likely to make a purchase based on their browsing history. This is like regression modelling, except that the numbers in the data are used to predict a categorical rather than a numerical output.\n",
        "\n",
        "Several machine learning algorithms exist for dealing with this situation. We're going to look at one of these––logistic regression––and explore how it can be used for predicting binary categories. This is when we have two outputs (pass or fail, cat or dog, sale or not-sale), with these outcomes represented as $0$ and $1$. It can also be adapted to deal with more than two categories, but we won't be looking at that case.\n",
        "\n",
        "The logic of binary logistic regression works as follows. The model uses a function called the logistic function to map the outputs of a regression into the range $(0,1)$. This is useful, because we can then interpret the results of the regression model as probabilities: if they are greater than $0.5$, the model predicts success; if they are less, the model predicts failure. In more detail, the logistic function has the following form:\n",
        "\n",
        "$$p(x) = \\frac{1}{1+e^{-t}}$$\n",
        "\n",
        "As you can see, the logistic function compresses its output into the $0$ to $1$ range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbab2470",
      "metadata": {
        "id": "bbab2470"
      },
      "outputs": [],
      "source": [
        "t = np.arange(-10, 10, 0.1)\n",
        "\n",
        "def logistic(x):\n",
        "    y = 1/(1+np.exp(-x))\n",
        "    return y\n",
        "\n",
        "sns.lineplot(x = t, y = [logistic(i) for i in t])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3bdd1c",
      "metadata": {
        "id": "4b3bdd1c"
      },
      "source": [
        "## Logistic regression in detail\n",
        "\n",
        "The key to understanding logistic regression comes with interpreting the parameter $t$. In logistic regression with a predictor variable $x$ and a predicted variable $y$, this corresponds to the equation for linear regression:\n",
        "\n",
        "$$y = \\beta_{0} + \\beta_{1}x$$\n",
        "\n",
        "where $\\beta_{0}$ is the $y$ intercept and $\\beta_{1}$ is the slope of the line of best fit.\n",
        "\n",
        "That is:\n",
        "\n",
        "$$p(x) = \\frac{1}{1+e^{-(\\beta_{0} + \\beta_{1}x)}}$$\n",
        "\n",
        "But how should we interpret this? To understand what's happening, we need to know about *log odds*. The log odds of an event is simply the logarithm of probility of that event occurring divided by the probability of it not occurring. That is, the log odds of an event of proability $p$ is:\n",
        "\n",
        "$$\\ln{\\frac{p}{1-p}}$$\n",
        "\n",
        "Imagine that in a class of students, the probability of a student passing an exam after attending 90% of classes is 0.75. The log odds of a student passing is therefore:\n",
        "\n",
        "$$\\ln{\\frac{0.75}{0.25}} = 1.09$$\n",
        "\n",
        "If this number is large, there is a high probability of success; if it is small, there is a low probability of success.\n",
        "\n",
        "For every value taken by the predictor variable $x$ (the percentage of classes attended) the log odds of a student passing can be calcualted. And because the log odds come in the form of a continuous number than a category, they can then be estimated using a standard linear regression model:\n",
        "\n",
        "$$y =\\ln{\\frac{p(1)}{p(0)}} = \\beta_{0} + \\beta_{1}x$$\n",
        "\n",
        "where $1$ represents passing the exam and $0$ represents failing. By estimating the $\\beta_{0}$ and $\\beta_{1}$ parameters and inserting them into the logistic function, we can therefore 'squash' our regression outputs into the (0,1) range, and interpret the results as probabilities. When greater than 0.5, the event is classed as one category (1 = pass) and less than 0.5 it's classed as another (0 = fail)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7418906",
      "metadata": {
        "id": "a7418906"
      },
      "source": [
        "## Evaluating model performance on categorical data\n",
        "\n",
        " Four metrics are typically used to assess model performance when there are categorical predictors: *precision*, *recall*, *accuracy*, and the *F1 score*. These all take values between $0$ and $1$, where $1$ is a perfect score.\n",
        "\n",
        "1. Precision: The precision score of a model measures how many of the positive predictions made by the model are in truth positive predictions. It's calculated as follows, where $TP$ means 'True Positive' and $FP$ means 'False Positive':\n",
        "\n",
        "$$Precision = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "2. Recall: The recall score of a model measures what fraction of the true positive cases the model manages to predict. It's calculated as follows, where $FN$ means 'False Negative':\n",
        "\n",
        "$$Recall = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "3. Accuracy: The accuracy score of model measures what fraction of correct predictions the model makes relative to the total number of predictions it makes. It's calculated as follows:\n",
        "\n",
        "$$Accuracy = \\frac{TN+TP}{TN+FP+TP+FN}$$\n",
        "\n",
        "4. F1: The F1 score of a model is the harmonic mean of precision and recall. It's used because it gives a way of capturing both metrics in a single score. It's calculated as follows:\n",
        "\n",
        "$$F1 = \\frac{2TP}{2TP+FP+FN}$$\n",
        "\n",
        "Which metric is chosen to evaluate the model depends on the purpose it's being used for. But in practice, the $F1$ is usually the best measure to use, as it gives a more rounded appreciation of model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249c30d5",
      "metadata": {
        "id": "249c30d5"
      },
      "source": [
        "## Logistic regression and NLP\n",
        "\n",
        "There are lots of situations in NLP where logistic regression is useful. For instance, we might want to classify whether or not an email is spam based on the language used, or classify comments as being toxic or not. Here, we're going to build a profanity detector. That is, we will train a logistic regression classifier to evaluate whether or not a word is a profanity, with 'profanity' being understood to include terms of racial, sexual, religious, and other forms of abuse. We will do this in the following way:\n",
        "\n",
        "1. Obtain a list of profanity words and non-profanity words and label them, where $1$ denotes a profanity and $0$ a non-profanity.\n",
        "2. Get word embeddings for these words using a pre-trained model from Twitter.\n",
        "3. Train a logistic regression model on a fraction of this dataset.\n",
        "4. Evaluate our model's performance on a the retained test sample.\n",
        "5. See how our model performs in the wild."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7004e28a",
      "metadata": {
        "id": "7004e28a"
      },
      "outputs": [],
      "source": [
        "model = api.load(\"glove-twitter-200\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb5d125",
      "metadata": {
        "id": "bbb5d125"
      },
      "outputs": [],
      "source": [
        "profanity_list = pd.read_csv(\"/content/AI_for_complex_problems/profanity_en.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f87307",
      "metadata": {
        "id": "a7f87307"
      },
      "outputs": [],
      "source": [
        "profanity_df = pd.DataFrame()\n",
        "profanity_df['word'] = [i for i in profanity_list['text'] if i in model.key_to_index]\n",
        "profanity_df['category'] = 1\n",
        "\n",
        "non_profanity = random.sample([i for i in data.index], len(profanity_df))\n",
        "notprofanity_df = pd.DataFrame()\n",
        "notprofanity_df['word'] = [i for i in non_profanity if i in model.key_to_index]\n",
        "notprofanity_df['category'] = 0\n",
        "\n",
        "dataset = pd.concat([profanity_df, notprofanity_df], axis = 0).reset_index(drop = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2a419e",
      "metadata": {
        "id": "5a2a419e"
      },
      "outputs": [],
      "source": [
        "vectors = []\n",
        "\n",
        "for i in dataset['word']:\n",
        "    vectors.append(model[i])\n",
        "\n",
        "vecs_df = pd.DataFrame(vectors)\n",
        "dataset = pd.concat([dataset, vecs_df], axis = 1)\n",
        "dataset = dataset.reset_index(drop = True)\n",
        "dataset = dataset.drop('word', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96783ef3",
      "metadata": {
        "id": "96783ef3"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe3a31a",
      "metadata": {
        "id": "7fe3a31a"
      },
      "outputs": [],
      "source": [
        "X = dataset.drop(['category'], axis = 'columns')\n",
        "y = dataset['category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ecf4a4",
      "metadata": {
        "id": "71ecf4a4"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train) #This fits the model to the training data\n",
        "preds = clf.predict(X_test) # This outputs the model predictions on the withheld test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c1ebf2",
      "metadata": {
        "id": "46c1ebf2"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e546c98",
      "metadata": {
        "id": "8e546c98"
      },
      "outputs": [],
      "source": [
        "test_cases = ['snot', 'scum', 'fuck', 'turd', 'piss', 'commie', 'nazi', 'bloody', 'peasant', 'bigot', 'apple',\\\n",
        "              'zebra', 'cloud', 'christian', 'muslim', 'jew', 'slut', 'hound', 'merde', \\\n",
        "              'scheisse', 'rando', 'mierda', 'mutt', 'sexy', 'angel', 'devil']\n",
        "\n",
        "profanity_words = [i.lower() for i in profanity_list['text']]\n",
        "\n",
        "test_cases = [i for i in test_cases if i not in profanity_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745c3243",
      "metadata": {
        "id": "745c3243"
      },
      "outputs": [],
      "source": [
        "in_data = []\n",
        "tests = []\n",
        "confidence = []\n",
        "\n",
        "for i in test_cases:\n",
        "    try:\n",
        "        tests.append(clf.predict(model[i.lower()].reshape(1, -1))[0])\n",
        "        confidence.append(clf.predict_proba(model[i.lower()].reshape(1, -1))[0].max())\n",
        "        in_data.append(i)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "tests_w = []\n",
        "for i in tests:\n",
        "    if i == 0:\n",
        "        tests_w.append('not profanity')\n",
        "    elif i == 1:\n",
        "        tests_w.append('profanity')\n",
        "tests_df = pd.DataFrame()\n",
        "tests_df['word'] = in_data\n",
        "tests_df['prediction'] = tests_w\n",
        "tests_df['confidence'] = confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e38c686",
      "metadata": {
        "id": "4e38c686"
      },
      "outputs": [],
      "source": [
        "tests_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429c766a",
      "metadata": {
        "id": "429c766a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}